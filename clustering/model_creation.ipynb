{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kprototypes.mod']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##### This notebook generates the MinMax scaler and the K-Prototypes model used for the prioritization clustering.\n",
    "    ##### Every time this notebook generates the model the results must be checked to make sure the labels are properly assigned\n",
    "    ##### in prioritized_corridors.csv, which is the data used in Power BI (this file is created in the model_prediction\n",
    "    ##### notebook). At the end of this notebook there's code to graph several strip plots that can help check the results.\n",
    "    ##### There's also code to perform a dimensionality reduction using UMAP and check the clusters visually in 2D\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from kmodes.kprototypes import KPrototypes\n",
    "import joblib\n",
    "from data_creation import data_for_clustering\n",
    "\n",
    "##### The methodology works by bringing 3-year data. Before running the script, the person who runs it can modify the date\n",
    "    ##### information below (year, month, day) and the script would automatically bring the 3-year data up to such date\n",
    "cluster_df = data_for_clustering(2021, 12, 31)\n",
    "##### We could save the data to make prioritization predictions using the model_prediction notebook\n",
    "#cluster_df.to_csv(\"raw_data_predict.csv\", index = False)\n",
    "\n",
    "##### We remove the corridors and the number of killed and injured vulnerable people to perform the clustering (the basic EDA\n",
    "    ##### mentioned in the data_creation notebook revealed the number of killed and injured vulnerable people were redundant)\n",
    "##### We normalize the continuous data \n",
    "cluster_df_norm = cluster_df[[\"HORARIO\", \"accidentes\", \"muertes\", \"heridos\", \"vulnerables\"]].copy()\n",
    "\n",
    "#del cluster_df\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "cluster_df_norm[[\"accidentes\", \"muertes\", \"heridos\"]] = scaler.fit_transform(cluster_df_norm[[\"accidentes\", \"muertes\", \\\n",
    "    \"heridos\"]])\n",
    "\n",
    "##### We save the scaler\n",
    "joblib.dump(scaler, \"scaler.mod\")\n",
    "\n",
    "##### We apply the clustering\n",
    "kproto = KPrototypes(n_clusters = 3, init = \"Cao\")\n",
    "clusters = kproto.fit_predict(cluster_df_norm, categorical = [0, 4])\n",
    "\n",
    "##### We save the model\n",
    "joblib.dump(kproto, \"kprototypes.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This is the code that helps check the clusters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "cluster_df[\"CONSTANT\"] = 0\n",
    "cluster_df = pd.concat((cluster_df, pd.DataFrame(clusters)), axis = 1)\n",
    "cluster_df.rename({0: \"labels\"}, axis = 1, inplace = True)\n",
    "\n",
    "fig, ax = plt.subplots(nrows = 3, ncols = 3, figsize = (17, 26))\n",
    "\n",
    "#Row 0 - column 0\n",
    "sns.stripplot(x = \"vulnerables\", y = \"accidentes\", hue = \"labels\", ax = ax[0, 0], data = cluster_df)\n",
    "ax[0, 0].set(xlabel = \"Severity level\", ylabel = \"Accidents\")\n",
    "ax[0, 0].legend(title = \"Labels\")\n",
    "# Row 0 - column 1\n",
    "sns.stripplot(x = \"vulnerables\", y = \"muertes\", hue = \"labels\", ax = ax[0, 1], data = cluster_df)\n",
    "ax[0, 1].set(xlabel = \"Severity level\", ylabel = \"Killed people\")\n",
    "ax[0, 1].legend(title = \"Labels\")\n",
    "# Row 0 - column 2\n",
    "sns.stripplot(x = \"vulnerables\", y = \"heridos\", hue = \"labels\", ax = ax[0, 2], data = cluster_df)\n",
    "ax[0, 2].set(xlabel = \"Severity level\", ylabel = \"Injured people\")\n",
    "ax[0, 2].legend(title = \"Labels\")\n",
    "# Row 1 - column 0\n",
    "sns.stripplot(x = \"HORARIO\", y = \"accidentes\", hue = \"labels\", ax = ax[1, 0], data = cluster_df)\n",
    "ax[1, 0].set(xlabel = \"\", ylabel = \"Accidents\")\n",
    "ax[1, 0].tick_params(axis = 'x', rotation = 45)\n",
    "ax[1, 0].legend(title = \"Labels\")\n",
    "# Row 1 - column 1\n",
    "sns.stripplot(x = \"HORARIO\", y = \"muertes\", hue = \"labels\", ax = ax[1, 1], data = cluster_df)\n",
    "ax[1, 1].set(xlabel = \"\", ylabel = \"Killed people\")\n",
    "ax[1, 1].tick_params(axis = 'x', rotation = 45)\n",
    "ax[1, 1].legend(title = \"Labels\")\n",
    "# Row 1 - column 2\n",
    "sns.stripplot(x = \"HORARIO\", y = \"heridos\", hue = \"labels\", ax = ax[1, 2], data = cluster_df)\n",
    "ax[1, 2].set(xlabel = \"\", ylabel = \"Injured people\")\n",
    "ax[1, 2].tick_params(axis = 'x', rotation = 45)\n",
    "ax[1, 2].legend(title = \"Labels\")\n",
    "# Row 2 - column 0\n",
    "sns.stripplot(x = \"CONSTANT\", y = \"accidentes\", hue = \"labels\", ax = ax[2, 0], data = cluster_df)\n",
    "ax[2, 0].set(xlabel = \"\", ylabel = \"Accidents\")\n",
    "ax[2, 0].set_xticks(())\n",
    "ax[2, 0].legend(title = \"Labels\")\n",
    "# Row 2 - column 1\n",
    "sns.stripplot(x = \"CONSTANT\", y = \"muertes\", hue = \"labels\", ax = ax[2, 1], data = cluster_df)\n",
    "ax[2, 1].set(xlabel = \"\", ylabel = \"Killed people\")\n",
    "ax[2, 1].set_xticks(())\n",
    "ax[2, 1].legend(title = \"Labels\")\n",
    "# Row 2 - column 2\n",
    "sns.stripplot(x = \"CONSTANT\", y = \"heridos\", hue = \"labels\", ax = ax[2, 2], data = cluster_df)\n",
    "ax[2, 2].set(xlabel = \"\", ylabel = \"Injured people\")\n",
    "ax[2, 2].set_xticks(())\n",
    "ax[2, 2].legend(title = \"Labels\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### This is the code to perform the dimensionality reduction\n",
    "    ##### This code is adapted from: https://antonsruberts.github.io/kproto-audience/\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap\n",
    "from sklearn import preprocessing\n",
    "\n",
    "##### Please note this assumes the clustering results were already analyzed. In this case, label 0 would be the highest\n",
    "    ##### priority level, followed by label 2 and then by label 1\n",
    "dictp = {0: \"Prioritized\", 1: \"NA\", 2: \"Complementary\"}\n",
    "cluster_df[\"clusters\"] = cluster_df[\"labels\"].map(dictp)\n",
    "\n",
    "cluster_df_umap = cluster_df[[\"HORARIO\", \"accidentes\", \"muertes\", \"heridos\", \"vulnerables\"]].copy()\n",
    "cluster_df_umap[\"HORARIO\"] = cluster_df_umap[\"HORARIO\"].astype(\"category\")\n",
    "cluster_df_umap[\"HORARIO\"] = cluster_df_umap[\"HORARIO\"].cat.codes.astype(\"category\")\n",
    "\n",
    "# Preprocessing numerical features\n",
    "numerical = cluster_df_umap.select_dtypes(exclude = \"category\")\n",
    "\n",
    "for c in numerical.columns:\n",
    "    pt = preprocessing.PowerTransformer()\n",
    "    numerical.loc[:, c] = pt.fit_transform(np.array(numerical[c]).reshape(-1, 1))\n",
    "    \n",
    "## Preprocessing categorical features\n",
    "categorical = cluster_df_umap.select_dtypes(include = \"category\")\n",
    "categorical = pd.get_dummies(categorical)\n",
    "\n",
    "# Percentage of categorical features is used as weight parameter in embeddings later\n",
    "categorical_weight = len(cluster_df_umap.select_dtypes(include = \"category\").columns) / cluster_df_umap.shape[1]\n",
    "\n",
    "# Embedding numerical & categorical features\n",
    "fit1 = umap.UMAP(metric = 'l2').fit(numerical)\n",
    "fit2 = umap.UMAP(metric = 'dice').fit(categorical)\n",
    "\n",
    "# Augmenting the numerical embedding with categorical embedding\n",
    "intersection = umap.general_simplicial_set_intersection(fit1.graph_, fit2.graph_, weight = categorical_weight)\n",
    "intersection = umap.reset_local_connectivity(intersection)\n",
    "embedding = umap.simplicial_set_embedding(fit1._raw_data, intersection, fit1.n_components, \n",
    "                                                fit1._initial_alpha, fit1._a, fit1._b, \n",
    "                                                fit1.repulsion_strength, fit1.negative_sample_rate, \n",
    "                                                200, 'random', np.random, fit1.metric, \n",
    "                                                fit1._metric_kwds, False, densmap_kwds = {}, output_dens = False)\n",
    "\n",
    "# We extract the embeddings for the UMAP plot\n",
    "embedding_values = np.array([pd.DataFrame(v) for v in embedding])\n",
    "umap_plot = embedding_values[0]\n",
    "\n",
    "# We plot the clusters in 2D\n",
    "plt.figure(figsize = (8, 6))\n",
    "sns.scatterplot(x = umap_plot[0], y = umap_plot[1],\n",
    "                hue = cluster_df[\"clusters\"],\n",
    "                palette = \"icefire\",\n",
    "                s = 50\n",
    ")\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "# Note that the order list below is based on an initial visualization to check the order in which the legend shows the\n",
    "    # clusters. The idea is to show the clusters in the legend according to the priority levels they represent, from the\n",
    "    # highest to the lowest. The list might need to be modified since UMAP delivers different results each time is run.\n",
    "order = [2, 0, 1]\n",
    "plt.legend([handles[i] for i in order], [labels[i] for i in order], title = \"Clusters\")\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel(\"\")\n",
    "plt.title(\"2D data visualization with UMAP\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14a8ab31f43739f8c61e31f0a764b4b9450b359fd4dd373e840d3db42f338d08"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
